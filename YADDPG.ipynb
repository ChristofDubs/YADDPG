{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tensorflow as tf\n",
    "import json, sys, os\n",
    "from os import path\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "\n",
    "Training RL agents using only the latest set of of state transitions S, A, R, S' generally leads to a problem known as \"catastrophic forgetting\" in which agents get too caught up on recent observations and overwrite their network weights with gradients generated from very specific, recent trajectories. The end result is that the agent learns, then swiftly forgets how to do a task.\n",
    "\n",
    "To break up that temporal correlation we use memory replay to buffer the last ```N``` state transitions and then randomly sample the buffer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, bufferSize=1e5):\n",
    "        self.buffer = deque([],bufferSize)\n",
    "\n",
    "    def recall(self, batchSize=1024):\n",
    "        batchSize = min(len(self.buffer), batchSize)\n",
    "        \n",
    "        batch = random.sample(self.buffer, batchSize)\n",
    "    \n",
    "        S = np.asarray([sample[0] for sample in batch]).reshape(batchSize, -1)\n",
    "        A = np.asarray([sample[1] for sample in batch]).reshape(batchSize, -1)\n",
    "        R = np.asarray([sample[2] for sample in batch]).reshape(batchSize)\n",
    "        S_dash = np.asarray([sample[3] for sample in batch]).reshape(batchSize, -1)\n",
    "        not_terminal = np.asarray([sample[4] for sample in batch]).reshape(batchSize)\n",
    "\n",
    "        return S, A, R, S_dash, not_terminal\n",
    "        \n",
    "    def store(self, state, action, reward, nextState, not_terminal):\n",
    "        self.buffer.append([state, action, reward, nextState, not_terminal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ornstein–Uhlenbeck Process\n",
    "\n",
    "DDPG generates a deterministic policy whereas it's usually better to allow for some stochasticity during training such that the agent explores its environment more effectively. DDPG is off-policy so technically we could use any sufficiently exploratory policy we like, but in accordance with the Deepmind paper, we'll just overlay an Ornstein–Uhlenbeck process on top of the current deterministic policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OU(object):\n",
    "    def __init__(self, dim, mu, theta, sigma):\n",
    "        self.dim = dim\n",
    "        self.mu, self.theta, self.sigma = mu, theta, sigma\n",
    "        self.noise_process = np.zeros(dim)\n",
    "\n",
    "    def get_noise(self):\n",
    "        self.noise_process = self.theta * (self.mu - self.noise_process) + self.sigma * np.random.randn(self.dim)\n",
    "        return self.noise_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the Tensorflow Networks\n",
    "\n",
    "## Network\n",
    "\n",
    "DDPG uses four networks; an actor, a critic and two target networks. The Network class generates the tf forward pass operation for a fully connected multilayer network using a spec dictionary to define the number of units and activation for each layer.\n",
    "\n",
    "## Target Networks\n",
    "\n",
    "Directly using the critic to generate target values during training causes the learning process to destabilise. To introduce some regularisation, DDPG uses slow moving copies of the actor and critic called target networks. The ```TargetNetwork``` class below implements these networks and defines their update operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, input_shape, spec, scope, trainable):\n",
    "        self.spec, self.scope, self.trainable = spec, scope, trainable\n",
    "        \n",
    "        self.get_forward_pass_op(tf.placeholder(dtype=tf.float32, shape=input_shape), False)\n",
    "        self.vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.scope)\n",
    "\n",
    "    def get_forward_pass_op(self, inputs, reuse=True):\n",
    "        with tf.variable_scope(self.scope, reuse=reuse):\n",
    "            for layer in self.spec:\n",
    "                inputs = tf.layers.dense(inputs, layer['units'], activation=layer['activation'], trainable=self.trainable)\n",
    "                \n",
    "        return inputs\n",
    "    \n",
    "    def sum_weights(self):\n",
    "        return tf.add_n([tf.nn.l2_loss(var) for var in self.vars if not 'bias' in var.name])\n",
    "    \n",
    "class TargetNetwork(Network):\n",
    "    def __init__(self, input_shape, spec, scope, trainable):\n",
    "        super(TargetNetwork, self).__init__(input_shape, spec, scope, trainable)\n",
    "        \n",
    "    def get_target_train_op(self, target_network, tau):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.vars):\n",
    "            update_op = var.assign(tau * target_network.vars[i] + (1 - tau) * var)\n",
    "            update_ops.append(update_op)\n",
    "\n",
    "        return tf.group(*update_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define an Agent\n",
    "\n",
    "The Agent represents the reinformcement learning algorithm. Like just about any RL algorithm it can be asked to make an action in a given state via ```act``` and it can observe S,A,R,S' tuples by storing them in its experience replay buffer.\n",
    "\n",
    "## Training the Critic\n",
    "\n",
    "The critic is trained in basically the same way as the Q-function from DQN. The loss function is defined as the td error plus a regularisation term on the critic network weights:\n",
    "\n",
    "\\begin{equation}\n",
    "loss = [ r + \\gamma Q_{target}(s',\\pi_{target}(s')) - Q(s,a) ] + \\sum{\\theta_Q}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "* $ s, a, r, s'$ are a state, action, reward and next state sampled from the experience replay buffer, \n",
    "* $ \\pi_{target} $ is the actor target network\n",
    "* $ Q $ and $ Q_{target} $ is the critic and critic target network respectively\n",
    "* $ \\theta_Q $ is the critic network weights\n",
    "\n",
    "The loss function is then minimised using an AdamOptimiser\n",
    "\n",
    "## Training the Actor\n",
    "\n",
    "To train the actor, we just use an AdamOptimiser to maximise the on-policy Q-function $ Q(s,\\pi(s)) $ (or equivilentlly minimise $ - Q(s,\\pi(s))$ ) along with a regularisation term on the actor network weights. To slow the actor's learning rate during training, the learning rate (the Adam step size) decays in proportion to $ \\epsilon $ to the power of the episode counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env, actor_network_spec, critic_network_spec, alpha=1e-3, alpha_decay=1, gamma=0.99, tau=1e-2, l2_reg=5e-7):\n",
    "        self.sess = tf.Session()\n",
    "        self.env = env\n",
    "        \n",
    "        state_dim = np.prod(env.observation_space.shape)\n",
    "        action_dim = np.prod(env.action_space.shape)\n",
    "        \n",
    "        # Ornstein–Uhlenbeck process\n",
    "        self.OU = OU(action_dim, 0.0, 0.15, 0.2)\n",
    "        \n",
    "        # experience replay\n",
    "        self.replay_memory = Experience(1e5)\n",
    "\n",
    "        # episode counter\n",
    "        self.episodes = tf.Variable(0.0, trainable=False)\n",
    "        self.episode_inc_op = self.episodes.assign_add(1)\n",
    "        \n",
    "        # tf placeholders\n",
    "        self.state_ph = tf.placeholder(dtype=tf.float32, shape=[None,state_dim])\n",
    "        self.action_ph = tf.placeholder(dtype=tf.float32, shape=[None,action_dim])\n",
    "        self.reward_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "        self.next_state_ph = tf.placeholder(dtype=tf.float32, shape=[None,state_dim])\n",
    "        self.is_not_terminal_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "        \n",
    "        # set up the networks\n",
    "        critic_network = Network([None, state_dim + action_dim], critic_network_spec, 'critic_net', trainable=True)\n",
    "        actor_network = Network([None, state_dim], actor_network_spec, 'actor_net', trainable=True)\n",
    "        slow_critic_network = TargetNetwork([None, state_dim + action_dim], critic_network_spec, 'slow_critic_net', trainable=False)\n",
    "        slow_actor_network = TargetNetwork([None, state_dim], actor_network_spec, 'slow_actor_net', trainable=False)\n",
    "\n",
    "        # set up the actor operation\n",
    "        self.policy_op = actor_network.get_forward_pass_op(self.state_ph) * (self.env.action_space.high - self.env.action_space.low)\n",
    "                \n",
    "        # set up the critic training operation\n",
    "        slow_target_next_actions = slow_actor_network.get_forward_pass_op(self.next_state_ph)\n",
    "        slow_q_values_next = slow_critic_network.get_forward_pass_op(tf.concat([self.next_state_ph, slow_target_next_actions], axis=1))\n",
    "        \n",
    "        critic_off_pol = critic_network.get_forward_pass_op(tf.concat([self.state_ph, self.action_ph], axis=1))\n",
    "\n",
    "        targets = tf.expand_dims(self.reward_ph, 1) + tf.expand_dims(self.is_not_terminal_ph, 1) * gamma * slow_q_values_next        \n",
    "        td_errors = targets - critic_off_pol\n",
    "        \n",
    "        critic_loss = tf.reduce_mean(tf.square(td_errors)) + l2_reg * critic_network.sum_weights()\n",
    "        self.critic_train_op = tf.train.AdamOptimizer(alpha * alpha_decay ** self.episodes).minimize(critic_loss)\n",
    "        \n",
    "        # set up the actor training operation\n",
    "        critic_on_pol = critic_network.get_forward_pass_op(tf.concat([self.state_ph, self.policy_op], axis=1))\n",
    "        actor_loss = -1 * tf.reduce_mean(critic_on_pol) + l2_reg * actor_network.sum_weights()\n",
    "        self.actor_train_op = tf.train.AdamOptimizer(alpha * alpha_decay ** self.episodes).minimize(actor_loss, var_list=actor_network.vars)\n",
    "        \n",
    "        # train slow networks\n",
    "        self.slow_actor_train_op = slow_actor_network.get_target_train_op(actor_network, tau=tau)\n",
    "        self.slow_critic_train_op = slow_critic_network.get_target_train_op(critic_network, tau=tau)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def act(self, state, initial_noise_scale=0.0, noise_decay=0.99):\n",
    "        action = self.sess.run(self.policy_op, feed_dict = {self.state_ph: state})\n",
    "\n",
    "        self.noise_scale = (initial_noise_scale * noise_decay ** self.sess.run(self.episodes)) * (self.env.action_space.high - self.env.action_space.low)\n",
    "        action += self.noise_scale * self.OU.get_noise()\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, batch_size=1024):\n",
    "        if len(self.replay_memory.buffer) >= batch_size:        \n",
    "            # grab N (s,a,r,s') tuples from replay memory\n",
    "            S, A, R, S_dash, not_terminal = self.replay_memory.recall(batch_size)\n",
    "\n",
    "            # update the critic and actor params using mean-square value error and deterministic policy gradient, respectively\n",
    "            self.sess.run(self.critic_train_op, feed_dict = {self.state_ph: S, self.action_ph: A, self.reward_ph: R, self.next_state_ph: S_dash, self.is_not_terminal_ph: not_terminal})\n",
    "            self.sess.run(self.actor_train_op, feed_dict = {self.state_ph: S})\n",
    "\n",
    "            # update slow actor and critic targets towards current actor and critic\n",
    "            self.sess.run([self.slow_actor_train_op, self.slow_critic_train_op])\n",
    "        \n",
    "    def increment_episode(self):\n",
    "        self.sess.run(self.episode_inc_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate!\n",
    "\n",
    "Finally, we simulate the agent using the Pendulum-v0 environment from the openAI gym. On every iteration of an episode for 1000 episodes, the agent acts, stores the S,A,R,S' tuple and trains its networks. After usually around 20-30 episodes it hones in on a good policy for swinging up and stabilising the pendulum and the total reward increases to around -200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env = wrappers.Monitor(env, './pendulum-v0-experiment', force=True)\n",
    "env.seed(0)\n",
    "\n",
    "critic_network_spec = [{'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 1, 'activation': None}]\n",
    "\n",
    "actor_network_spec =  [{'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': np.prod(env.action_space.shape), 'activation': tf.nn.tanh}]\n",
    "\n",
    "agent = Agent(env, \n",
    "              actor_network_spec,\n",
    "              critic_network_spec,\n",
    "              alpha=1e-3,\n",
    "              alpha_decay=1,\n",
    "              gamma=0.99,\n",
    "              tau=1e-2, \n",
    "              l2_reg=5e-7)\n",
    "\n",
    "num_episodes = 1000\n",
    "max_steps_ep = 10000\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    steps_in_ep = 0\n",
    "\n",
    "    # Initial state\n",
    "    s_t = env.reset()[None]\n",
    "    \n",
    "    if ep % 10 == 0: env.render()\n",
    "\n",
    "    for t in range(max_steps_ep):\n",
    "        a_t = agent.act(s_t)\n",
    "\n",
    "        # take step\n",
    "        s_t_plus_1, r_t, done, _info = env.step(a_t)\n",
    "                \n",
    "        if ep % 10 == 0: env.render()\n",
    "        total_reward += r_t\n",
    "\n",
    "        agent.replay_memory.store(s_t, a_t, r_t, s_t_plus_1.T, 0.0 if done else 1.0)\n",
    "        agent.train()\n",
    "            \n",
    "        s_t = s_t_plus_1.T\n",
    "        steps_in_ep += 1\n",
    "\n",
    "        if done: break\n",
    "\n",
    "    agent.increment_episode()\n",
    "    print('Episode %2i, Reward: %7.3f, Steps: %i'%(ep, total_reward, steps_in_ep))\n",
    "\n",
    "# Finalize and upload results\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

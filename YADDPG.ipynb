{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tensorflow as tf\n",
    "import json, sys, os\n",
    "from os import path\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, bufferSize=1e5):\n",
    "        self.buffer = deque([],bufferSize)\n",
    "\n",
    "    def recall(self, batchSize=1024):\n",
    "        batchSize = min(len(self.buffer), batchSize)\n",
    "        \n",
    "        batch = random.sample(self.buffer, batchSize)\n",
    "    \n",
    "        S = np.asarray([sample[0] for sample in batch]).reshape(batchSize, -1)\n",
    "        A = np.asarray([sample[1] for sample in batch]).reshape(batchSize, -1)\n",
    "        R = np.asarray([sample[2] for sample in batch]).reshape(batchSize)\n",
    "        S_dash = np.asarray([sample[3] for sample in batch]).reshape(batchSize, -1)\n",
    "        not_terminal = np.asarray([sample[4] for sample in batch]).reshape(batchSize)\n",
    "\n",
    "        return S, A, R, S_dash, not_terminal\n",
    "        \n",
    "    def store(self, state, action, reward, nextState, not_terminal):\n",
    "        self.buffer.append([state, action, reward, nextState, not_terminal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OU(object):\n",
    "    def __init__(self, dim, mu, theta, sigma):\n",
    "        self.dim = dim\n",
    "        self.mu, self.theta, self.sigma = mu, theta, sigma\n",
    "        self.noise_process = np.zeros(dim)\n",
    "\n",
    "    def get_noise(self):\n",
    "        self.noise_process = self.theta * (self.mu - self.noise_process) + self.sigma * np.random.randn(self.dim)\n",
    "        return self.noise_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, input_shape, spec, scope, trainable):\n",
    "        self.spec, self.scope, self.trainable = spec, scope, trainable\n",
    "        \n",
    "        self.get_forward_pass_op(tf.placeholder(dtype=tf.float32, shape=input_shape), False)\n",
    "        self.vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.scope)\n",
    "\n",
    "    def get_forward_pass_op(self, inputs, reuse=True):\n",
    "        with tf.variable_scope(self.scope, reuse=reuse):\n",
    "            for layer in self.spec:\n",
    "                inputs = tf.layers.dense(inputs, layer['units'], activation=layer['activation'], trainable=self.trainable)\n",
    "                \n",
    "        return inputs\n",
    "    \n",
    "    def sum_weights(self):\n",
    "        return tf.add_n([tf.nn.l2_loss(var) for var in self.vars if not 'bias' in var.name])\n",
    "    \n",
    "class TargetNetwork(Network):\n",
    "    def __init__(self, input_shape, spec, scope, trainable):\n",
    "        super(TargetNetwork, self).__init__(input_shape, spec, scope, trainable)\n",
    "        \n",
    "    def get_target_train_op(self, target_network, tau):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.vars):\n",
    "            update_op = var.assign(tau * target_network.vars[i] + (1 - tau) * var)\n",
    "            update_ops.append(update_op)\n",
    "\n",
    "        return tf.group(*update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env, actor_network_spec, critic_network_spec, alpha=1e-3, alpha_decay=1, gamma=0.99, tau=1e-2, l2_reg=5e-7):\n",
    "        self.sess = tf.Session()\n",
    "        self.env = env\n",
    "        \n",
    "        state_dim = np.prod(env.observation_space.shape)\n",
    "        action_dim = np.prod(env.action_space.shape)\n",
    "        \n",
    "        # Ornsteinâ€“Uhlenbeck process\n",
    "        self.OU = OU(action_dim, 0.0, 0.15, 0.2)\n",
    "        \n",
    "        # experience replay\n",
    "        self.replay_memory = Experience(1e5)\n",
    "\n",
    "        # episode counter\n",
    "        self.episodes = tf.Variable(0.0, trainable=False)\n",
    "        self.episode_inc_op = self.episodes.assign_add(1)\n",
    "        \n",
    "        # tf placeholders\n",
    "        self.state_ph = tf.placeholder(dtype=tf.float32, shape=[None,state_dim])\n",
    "        self.action_ph = tf.placeholder(dtype=tf.float32, shape=[None,action_dim])\n",
    "        self.reward_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "        self.next_state_ph = tf.placeholder(dtype=tf.float32, shape=[None,state_dim])\n",
    "        self.is_not_terminal_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "        \n",
    "        # set up the networks\n",
    "        critic_network = Network([None, state_dim + action_dim], critic_network_spec, 'critic_net', trainable=True)\n",
    "        actor_network = Network([None, state_dim], actor_network_spec, 'actor_net', trainable=True)\n",
    "        slow_critic_network = TargetNetwork([None, state_dim + action_dim], critic_network_spec, 'slow_critic_net', trainable=False)\n",
    "        slow_actor_network = TargetNetwork([None, state_dim], actor_network_spec, 'slow_actor_net', trainable=False)\n",
    "\n",
    "        # actors\n",
    "        self.policy_op = actor_network.get_forward_pass_op(self.state_ph) * (self.env.action_space.high - self.env.action_space.low)\n",
    "        slow_target_next_actions = slow_actor_network.get_forward_pass_op(self.next_state_ph)\n",
    "        \n",
    "        # critics\n",
    "        critic_off_pol = critic_network.get_forward_pass_op(tf.concat([self.state_ph, self.action_ph], axis=1))\n",
    "        critic_on_pol = critic_network.get_forward_pass_op(tf.concat([self.state_ph, self.policy_op], axis=1))\n",
    "        slow_q_values_next = slow_critic_network.get_forward_pass_op(tf.concat([self.next_state_ph, slow_target_next_actions], axis=1))\n",
    "        \n",
    "        # train critic\n",
    "        targets = tf.expand_dims(self.reward_ph, 1) + tf.expand_dims(self.is_not_terminal_ph, 1) * gamma * slow_q_values_next\n",
    "        td_errors = targets - critic_off_pol\n",
    "        critic_loss = tf.reduce_mean(tf.square(td_errors)) + l2_reg * critic_network.sum_weights()\n",
    "        self.critic_train_op = tf.train.AdamOptimizer(alpha * alpha_decay ** self.episodes).minimize(critic_loss)\n",
    "        \n",
    "        # train actor\n",
    "        actor_loss = -1 * tf.reduce_mean(critic_on_pol) + l2_reg * actor_network.sum_weights()\n",
    "        self.actor_train_op = tf.train.AdamOptimizer(alpha * alpha_decay ** self.episodes).minimize(actor_loss, var_list=actor_network.vars)\n",
    "        \n",
    "        # train slow networks\n",
    "        self.slow_actor_train_op = slow_actor_network.get_target_train_op(actor_network, tau=tau)\n",
    "        self.slow_critic_train_op = slow_critic_network.get_target_train_op(critic_network, tau=tau)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def act(self, state, initial_noise_scale=0.0, noise_decay=0.99):\n",
    "        action = self.sess.run(self.policy_op, feed_dict = {self.state_ph: state})\n",
    "\n",
    "        self.noise_scale = (initial_noise_scale * noise_decay ** self.sess.run(self.episodes)) * (self.env.action_space.high - self.env.action_space.low)\n",
    "        action += self.noise_scale * self.OU.get_noise()\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, batch_size=1024):\n",
    "        if len(self.replay_memory.buffer) >= batch_size:        \n",
    "            # grab N (s,a,r,s') tuples from replay memory\n",
    "            S, A, R, S_dash, not_terminal = self.replay_memory.recall(batch_size)\n",
    "\n",
    "            # update the critic and actor params using mean-square value error and deterministic policy gradient, respectively\n",
    "            self.sess.run(self.critic_train_op, feed_dict = {self.state_ph: S, self.action_ph: A, self.reward_ph: R, self.next_state_ph: S_dash, self.is_not_terminal_ph: not_terminal})\n",
    "            self.sess.run(self.actor_train_op, feed_dict = {self.state_ph: S})\n",
    "\n",
    "            # update slow actor and critic targets towards current actor and critic\n",
    "            self.sess.run([self.slow_actor_train_op, self.slow_critic_train_op])\n",
    "        \n",
    "    def increment_episode(self):\n",
    "        self.sess.run(self.episode_inc_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0, Reward: -1459.679, Steps: 200\n",
      "Episode  1, Reward: -1575.499, Steps: 200\n",
      "Episode  2, Reward: -1512.019, Steps: 200\n",
      "Episode  3, Reward: -1647.590, Steps: 200\n",
      "Episode  4, Reward: -1465.100, Steps: 200\n",
      "Episode  5, Reward: -1376.639, Steps: 200\n",
      "Episode  6, Reward: -1521.474, Steps: 200\n",
      "Episode  7, Reward: -1403.017, Steps: 200\n",
      "Episode  8, Reward: -1529.288, Steps: 200\n",
      "Episode  9, Reward: -1589.592, Steps: 200\n",
      "Episode 10, Reward: -1587.410, Steps: 200\n",
      "Episode 11, Reward: -1219.870, Steps: 200\n",
      "Episode 12, Reward: -1702.513, Steps: 200\n",
      "Episode 13, Reward: -1566.213, Steps: 200\n",
      "Episode 14, Reward: -690.815, Steps: 200\n",
      "Episode 15, Reward: -1493.743, Steps: 200\n",
      "Episode 16, Reward: -1017.100, Steps: 200\n",
      "Episode 17, Reward: -794.358, Steps: 200\n",
      "Episode 18, Reward: -1140.858, Steps: 200\n",
      "Episode 19, Reward: -1237.183, Steps: 200\n",
      "Episode 20, Reward: -1309.831, Steps: 200\n",
      "Episode 21, Reward: -1359.508, Steps: 200\n",
      "Episode 22, Reward: -1152.257, Steps: 200\n",
      "Episode 23, Reward: -1171.089, Steps: 200\n",
      "Episode 24, Reward: -875.921, Steps: 200\n",
      "Episode 25, Reward: -878.098, Steps: 200\n",
      "Episode 26, Reward: -814.562, Steps: 200\n",
      "Episode 27, Reward: -1194.194, Steps: 200\n",
      "Episode 28, Reward: -649.026, Steps: 200\n",
      "Episode 29, Reward: -762.034, Steps: 200\n",
      "Episode 30, Reward: -274.895, Steps: 200\n",
      "Episode 31, Reward: -387.034, Steps: 200\n",
      "Episode 32, Reward: -11.191, Steps: 200\n",
      "Episode 33, Reward: -266.563, Steps: 200\n",
      "Episode 34, Reward: -1743.328, Steps: 200\n",
      "Episode 35, Reward: -1704.018, Steps: 200\n",
      "Episode 36, Reward: -1650.438, Steps: 200\n",
      "Episode 37, Reward: -1240.988, Steps: 200\n",
      "Episode 38, Reward: -131.379, Steps: 200\n",
      "Episode 39, Reward: -262.063, Steps: 200\n",
      "Episode 40, Reward: -1504.975, Steps: 200\n",
      "Episode 41, Reward: -1504.875, Steps: 200\n",
      "Episode 42, Reward: -1524.566, Steps: 200\n",
      "Episode 43, Reward:  -7.283, Steps: 200\n",
      "Episode 44, Reward: -129.186, Steps: 200\n",
      "Episode 45, Reward: -126.971, Steps: 200\n",
      "Episode 46, Reward: -1387.696, Steps: 200\n",
      "Episode 47, Reward: -1507.447, Steps: 200\n",
      "Episode 48, Reward:  -8.303, Steps: 200\n",
      "Episode 49, Reward:  -6.925, Steps: 200\n",
      "Episode 50, Reward: -705.026, Steps: 200\n",
      "Episode 51, Reward: -1620.784, Steps: 200\n",
      "Episode 52, Reward: -1523.350, Steps: 200\n",
      "Episode 53, Reward: -1570.772, Steps: 200\n",
      "Episode 54, Reward: -146.141, Steps: 200\n",
      "Episode 55, Reward: -1494.272, Steps: 200\n",
      "Episode 56, Reward: -1539.047, Steps: 200\n",
      "Episode 57, Reward: -1503.017, Steps: 200\n",
      "Episode 58, Reward: -133.756, Steps: 200\n",
      "Episode 59, Reward: -1608.731, Steps: 200\n",
      "Episode 60, Reward: -133.259, Steps: 200\n",
      "Episode 61, Reward: -1517.072, Steps: 200\n",
      "Episode 62, Reward: -129.795, Steps: 200\n",
      "Episode 63, Reward: -129.896, Steps: 200\n",
      "Episode 64, Reward: -123.579, Steps: 200\n",
      "Episode 65, Reward: -130.439, Steps: 200\n",
      "Episode 66, Reward:  -7.093, Steps: 200\n",
      "Episode 67, Reward: -413.661, Steps: 200\n",
      "Episode 68, Reward: -1497.769, Steps: 200\n",
      "Episode 69, Reward: -127.903, Steps: 200\n",
      "Episode 70, Reward: -263.569, Steps: 200\n",
      "Episode 71, Reward: -264.989, Steps: 200\n",
      "Episode 72, Reward: -1499.928, Steps: 200\n",
      "Episode 73, Reward:  -7.491, Steps: 200\n",
      "Episode 74, Reward: -259.688, Steps: 200\n",
      "Episode 75, Reward:  -7.326, Steps: 200\n",
      "Episode 76, Reward: -120.081, Steps: 200\n",
      "Episode 77, Reward: -129.766, Steps: 200\n",
      "Episode 78, Reward: -260.307, Steps: 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0cd0c004ba5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_t_plus_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0ms_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_t_plus_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d93ede4868ba>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# grab N (s,a,r,s') tuples from replay memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_dash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# update the critic and actor params using mean-square value error and deterministic policy gradient, respectively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ca152cfe93c8>\u001b[0m in \u001b[0;36mrecall\u001b[0;34m(self, batchSize)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mS_dash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mnot_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_dash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env = wrappers.Monitor(env, './pendulum-v0-experiment', force=True)\n",
    "env.seed(0)\n",
    "\n",
    "critic_network_spec = [{'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 1, 'activation': None}]\n",
    "\n",
    "actor_network_spec =  [{'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': 8, 'activation': tf.nn.relu},\n",
    "                       {'units': np.prod(env.action_space.shape), 'activation': tf.nn.tanh}]\n",
    "\n",
    "agent = Agent(env, \n",
    "              actor_network_spec,\n",
    "              critic_network_spec,\n",
    "              alpha=1e-3,\n",
    "              alpha_decay=1,\n",
    "              gamma=0.99,\n",
    "              tau=1e-2, \n",
    "              l2_reg=5e-7)\n",
    "\n",
    "num_episodes = 1000\n",
    "max_steps_ep = 10000\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    steps_in_ep = 0\n",
    "\n",
    "    # Initial state\n",
    "    s_t = env.reset()[None]\n",
    "    \n",
    "    if ep % 10 == 0: env.render()\n",
    "\n",
    "    for t in range(max_steps_ep):\n",
    "        a_t = agent.act(s_t)\n",
    "\n",
    "        # take step\n",
    "        s_t_plus_1, r_t, done, _info = env.step(a_t)\n",
    "                \n",
    "        if ep % 10 == 0: env.render()\n",
    "        total_reward += r_t\n",
    "\n",
    "        agent.replay_memory.store(s_t, a_t, r_t, s_t_plus_1.T, 0.0 if done else 1.0)\n",
    "        agent.train()\n",
    "            \n",
    "        s_t = s_t_plus_1.T\n",
    "        steps_in_ep += 1\n",
    "\n",
    "        if done: break\n",
    "\n",
    "    agent.increment_episode()\n",
    "    print('Episode %2i, Reward: %7.3f, Steps: %i'%(ep, total_reward, steps_in_ep))\n",
    "\n",
    "# Finalize and upload results\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
